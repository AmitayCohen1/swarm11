# Vision: Research That Figures Things Out

## The Example

A founder messages his agent:

> "I need a developer relations lead. Someone technical enough to earn respect from senior engineers, but who actually enjoys being on Twitter. We sell to platform teams. Go."

The agent starts with the obvious: LinkedIn searches for "Developer Advocate" and "DevRel" at great developer-first companies — Datadog, Temporal, Langchain. It finds hundreds of profiles. But job titles don't reveal who's actually good at this.

It pivots to signal over credentials. It searches YouTube for conference talks. It finds 50+ speakers, then filters for those with talks that have strong engagement.

It cross-references those speakers with Twitter. Half have inactive accounts or just retweet their employer's blog posts. Not what we want. But a dozen have real followings — they post real opinions, reply to people, and get engagement from developers. And their posts have real taste.

The agent narrows further. It checks who's been posting less frequently in the last three months. A drop in activity sometimes signals disengagement from their current role. Three names surface.

It researches those three. One just announced a new role — too late. One is a founder of a company that just raised funding — not leaving. The third is a senior DevRel at a Series D company that just did layoffs in marketing. Her last talk was about exactly the platform engineering space the startup targets. She has 14k Twitter followers and posts memes that actual engineers engage with. She hasn't updated her LinkedIn in two months.

The agent drafts an email acknowledging her recent talk, the overlap with the startup's ICP, and a specific note about the creative freedom a smaller team offers. It suggests a casual conversation, not a pitch.

**Total time: 31 minutes.** The founder has a shortlist of one instead of a JD posted to a job board.

---

## What This Means

This is what it means to **figure things out**. Navigating ambiguity to accomplish a goal – forming hypotheses, testing them, hitting dead ends, and pivoting until something clicks.

The agent didn't follow a script. It ran the same loop a great recruiter runs in their head, except it did it tirelessly in 31 minutes, without being told how.

### The Loop

1. **Start broad** — find the obvious candidates
2. **Hit a wall** — realize credentials don't equal quality
3. **Pivot to signal** — find where quality actually shows up
4. **Filter ruthlessly** — cross-reference, look for disqualifiers
5. **Narrow to actionable** — get specific names you can act on
6. **Stop when the user can ACT** — not when you've "covered everything"

### What Makes It Work

- **Hypotheses, not scripts** — "Maybe conference speakers are better signals than job titles"
- **Dead ends are data** — "Half have inactive Twitter accounts" tells you something
- **Specificity over coverage** — One great candidate beats a list of 50 maybes
- **Context awareness** — Layoffs at her company + decreased posting = timing signal

---

## The Hard Part

To be clear: agents still fail. They hallucinate, lose context, and sometimes charge confidently down exactly the wrong path.

But the trajectory is unmistakable, and the failures are increasingly fixable.

The goal isn't perfection. It's building an agent that thinks like your smartest, most resourceful colleague — one who doesn't need a script, just a goal.
